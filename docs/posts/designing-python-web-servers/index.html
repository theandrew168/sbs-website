<!doctype html><html lang=en><head><title>Designing Python Web Servers Â· Shallow Brook Software
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Andrew Dailey"><meta name=description content="I&rsquo;ve spent the last few days designing and benchmarking extremely minimal pure-Python web servers. There is a tech myth / superstition that exposing python web servers to the internet is a bad idea but I&rsquo;ve seen little to no evidence as to why this is supposedly the case. Are they too slow? Are they too insecure for some reason? One thing is for sure: performance must always be measured, not guessed."><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Designing Python Web Servers"><meta name=twitter:description content="I&rsquo;ve spent the last few days designing and benchmarking extremely minimal pure-Python web servers. There is a tech myth / superstition that exposing python web servers to the internet is a bad idea but I&rsquo;ve seen little to no evidence as to why this is supposedly the case. Are they too slow? Are they too insecure for some reason? One thing is for sure: performance must always be measured, not guessed."><meta property="og:url" content="https://shallowbrooksoftware.com/posts/designing-python-web-servers/"><meta property="og:site_name" content="Shallow Brook Software"><meta property="og:title" content="Designing Python Web Servers"><meta property="og:description" content="I&amp;rsquo;ve spent the last few days designing and benchmarking extremely minimal pure-Python web servers. There is a tech myth / superstition that exposing python web servers to the internet is a bad idea but I&amp;rsquo;ve seen little to no evidence as to why this is supposedly the case. Are they too slow? Are they too insecure for some reason? One thing is for sure: performance must always be measured, not guessed."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-09-02T00:00:00+00:00"><meta property="article:modified_time" content="2020-09-02T00:00:00+00:00"><meta property="article:tag" content="Python"><link rel=canonical href=https://shallowbrooksoftware.com/posts/designing-python-web-servers/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.577e3c5ead537873430da16f0964b754a120fd87c4e2203a00686e7c75b51378.css integrity="sha256-V348Xq1TeHNDDaFvCWS3VKEg/YfE4iA6AGhufHW1E3g=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://shallowbrooksoftware.com/>Shallow Brook Software
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://shallowbrooksoftware.com/posts/designing-python-web-servers/>Designing Python Web Servers</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2020-09-02T00:00:00Z>September 2, 2020
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
6-minute read</span></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/python/>Python</a></span></div></div></header><div class=post-content><p>I&rsquo;ve spent the last few days designing and benchmarking extremely minimal pure-Python web servers.
There is a tech myth / superstition that exposing python web servers to the internet is a bad idea but I&rsquo;ve seen little to no evidence as to why this is supposedly the case.
Are they too slow?
Are they too insecure for some reason?
One thing is for sure: performance must always be measured, not guessed.</p><h1 id=designs>Designs
<a class=heading-link href=#designs><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>The designs I tested can be grouped into 4 broad categories: sequential, forking, threading, and asynchronous.</p><h3 id=sequential>Sequential
<a class=heading-link href=#sequential><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Sequential servers are the simplest and most straightfoward.
They process all clients in the order they arrive, one at a time.
This means that every client has to wait in a potentially very long line before they get service.
I expect this approach to be mediocre.</p><h3 id=forking>Forking
<a class=heading-link href=#forking><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Forking servers spawn a new operating system process for each client that arrives.
This means that clients don&rsquo;t have to wait on each other but more of the server&rsquo;s resources will be consumed.
Additionally, spawning new processes tends to have noticable overhead when many clients connect rapidly.
I expect this approach to be visibly affected by this overhead.</p><h3 id=threading>Threading
<a class=heading-link href=#threading><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Threading servers are similar to forking servers except that they spawn a thread to handle each client instead of a process.
Threads tend to have lower overhead than processes but aren&rsquo;t quite as independent when it comes to grabbing time on the CPU.
They share memory as well as some other server resources.
I expect this approach to perform well since web servers tend be bound by IO, not the CPU.</p><h3 id=asynchronous>Asynchronous
<a class=heading-link href=#asynchronous><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Asynchronous servers double down on the IO-heavy workload of web servers.
They keep all client connections together in such a way that no connections are ever waited on.
Only when data is ready to be read from a client does the server &ldquo;wake up&rdquo; and process it.
I expect this approach to perform very well since it is tailored to fit IO-bound workloads.</p><h1 id=benchmark>Benchmark
<a class=heading-link href=#benchmark><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>The servers were tested whilst running on a minimal <a href=https://www.digitalocean.com/ class=external-link target=_blank rel=noopener>DigitalOcean</a> Droplet (1 CPU, 1GB RAM).
All of them are using a TCP listen backlog of 128.
Printing each request to stdout is disabled during the benchmark.
For the actual load testing, I used a tool called <a href=https://github.com/rakyll/hey class=external-link target=_blank rel=noopener>hey</a>.</p><p>It performs 2000 requests spread across 50 concurrent connections:</p><pre tabindex=0><code>hey -n 2000 -c 50 http://&lt;hostname&gt;
</code></pre><h1 id=results>Results
<a class=heading-link href=#results><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>I started with a load test against <a href=https://nginx.org/en/ class=external-link target=_blank rel=noopener>NGINX</a> as a baseline.
I then measured all of the other servers in the same way.</p><p>RPS stands for &ldquo;Requests per second&rdquo; and latencies are measured in seconds.
The column &ldquo;% NGINX (RPS)&rdquo; is extra cool in my opinion.
It clearly shows how much room for improvement exists within each design.</p><table><thead><tr><th>Server</th><th>RPS</th><th>Mean Latency</th><th>Mode Latency</th><th>Worst Latency</th><th>% NGINX (RPS)</th></tr></thead><tbody><tr><td><a href=https://github.com/nginx/nginx class=external-link target=_blank rel=noopener>NGINX</a></td><td>672.36</td><td>0.07</td><td>0.11</td><td>0.64</td><td>100.00</td></tr><tr><td><a href=https://github.com/theandrew168/web-server-designs/blob/master/sequential1.py class=external-link target=_blank rel=noopener>sequential1.py</a></td><td>54.23</td><td>0.84</td><td>0.92</td><td>8.10</td><td>8.07</td></tr><tr><td><a href=https://github.com/theandrew168/web-server-designs/blob/master/sequential2.py class=external-link target=_blank rel=noopener>sequential2.py</a></td><td>52.77</td><td>0.84</td><td>0.91</td><td>7.85</td><td>7.85</td></tr><tr><td><a href=https://github.com/theandrew168/web-server-designs/blob/master/sequential3.py class=external-link target=_blank rel=noopener>sequential3.py</a></td><td>45.71</td><td>0.78</td><td>1.64</td><td>15.88</td><td>6.80</td></tr><tr><td><a href=https://github.com/theandrew168/web-server-designs/blob/master/forking1.py class=external-link target=_blank rel=noopener>forking1.py</a></td><td>8.70</td><td>5.60</td><td>5.14</td><td>10.14</td><td>1.29</td></tr><tr><td><a href=https://github.com/theandrew168/web-server-designs/blob/master/forking2.py class=external-link target=_blank rel=noopener>forking2.py</a></td><td>81.83</td><td>0.54</td><td>0.87</td><td>7.60</td><td>12.17</td></tr><tr><td><a href=https://github.com/theandrew168/web-server-designs/blob/master/threading1.py class=external-link target=_blank rel=noopener>threading1.py</a></td><td>78.71</td><td>0.52</td><td>0.87</td><td>7.52</td><td>11.71</td></tr><tr><td><a href=https://github.com/theandrew168/web-server-designs/blob/master/threading2.py class=external-link target=_blank rel=noopener>threading2.py</a></td><td>79.06</td><td>0.51</td><td>0.88</td><td>7.73</td><td>11.76</td></tr><tr><td><a href=https://github.com/theandrew168/web-server-designs/blob/master/processpool.py class=external-link target=_blank rel=noopener>processpool.py</a></td><td>48.52</td><td>0.71</td><td>1.01</td><td>8.81</td><td>7.22</td></tr><tr><td><a href=https://github.com/theandrew168/web-server-designs/blob/master/threadpool.py class=external-link target=_blank rel=noopener>threadpool.py</a></td><td>70.04</td><td>0.62</td><td>0.87</td><td>7.49</td><td>10.42</td></tr><tr><td><a href=https://github.com/theandrew168/web-server-designs/blob/master/nonblocking.py class=external-link target=_blank rel=noopener>nonblocking.py</a></td><td>79.33</td><td>0.53</td><td>0.63</td><td>5.23</td><td>11.80</td></tr><tr><td><a href=https://github.com/theandrew168/web-server-designs/blob/master/async.py class=external-link target=_blank rel=noopener>async.py</a></td><td>82.97</td><td>0.53</td><td>0.84</td><td>7.35</td><td>12.34</td></tr></tbody></table><h1 id=analysis>Analysis
<a class=heading-link href=#analysis><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Pretty interesting stuff!
Some designs were awful, some were middle-of-the-road, and some were decent.
As expected, the simple sequential servers were quite average and all floated around that 50 RPS range.
The <code>forking1.py</code> design was the worst by far.
Spawning a new process per client connection really does add a large amount of overhead.
I&rsquo;m really not sure why it was so much worse than <code>forking2.py</code> which uses Python&rsquo;s builtin <a href=https://docs.python.org/3/library/socketserver.html#socketserver.ForkingTCPServer class=external-link target=_blank rel=noopener>ForkingTCPServer</a> (this supposedly does the same thing).
By watching the actually system processes, the latter didn&rsquo;t seem to be creating a new process for each client as it describes.
I wonder if it has some sort of internal &ldquo;max processes&rdquo; limit to prevent things from getting out of hand.</p><p>All of the threading servers did well.
I&rsquo;m not surprised by this given that a web server&rsquo;s workload tends to be very IO-bound.
If the workload was more CPU-bound then I&rsquo;d expect to the threading designs rank a bit lower.
The <code>nonblocking.py</code> and <code>async.py</code> servers also performed well for a similar reason: an IO-bound workload.
These two designs both multiplex the socket IO handling around which clients are ready for action <em>right now</em>.
The two &ldquo;pool&rdquo; designs did decent but not as well as their unpooled counterparts.
The overhead of managing the pool must be coming into play here.</p><h1 id=existing-servers>Existing Servers
<a class=heading-link href=#existing-servers><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Just for completeness, I wanted to see how a few existing Python-based web servers stacked up against my quick and dirty designs.
Unsurprisingly, they beat me by a long shot!
Most of them did, at least.</p><table><thead><tr><th>Server</th><th>RPS</th><th>Mean Latency</th><th>Mode Latency</th><th>Worst Latency</th><th>% NGINX (RPS)</th></tr></thead><tbody><tr><td><a href=https://github.com/nginx/nginx class=external-link target=_blank rel=noopener>NGINX</a></td><td>672.36</td><td>0.07</td><td>0.11</td><td>0.64</td><td>100.00</td></tr><tr><td><a href=https://docs.gunicorn.org/en/latest/design.html#sync-workers class=external-link target=_blank rel=noopener>Gunicorn[sync]</a></td><td>46.31</td><td>0.99</td><td>0.95</td><td>8.23</td><td>6.89</td></tr><tr><td><a href=https://docs.gunicorn.org/en/latest/design.html#async-workers class=external-link target=_blank rel=noopener>Gunicorn[gevent]</a></td><td>338.36</td><td>0.14</td><td>0.09</td><td>0.43</td><td>50.32</td></tr><tr><td><a href=https://docs.pylonsproject.org/projects/waitress/en/stable/index.html class=external-link target=_blank rel=noopener>Waitress</a></td><td>449.07</td><td>0.10</td><td>0.09</td><td>0.42</td><td>66.79</td></tr></tbody></table><p>The synchronous Gunicorn did about as well as my own sequential servers which is to be expected.
Gunicorn with <a href=http://www.gevent.org/ class=external-link target=_blank rel=noopener>gevent</a> based workers did extremely well.
Gevent is a well-optimized implementation of asynchronous IO with its core event loop written in C (as either <a href=http://software.schmorp.de/pkg/libev.html class=external-link target=_blank rel=noopener>libev</a> or <a href=http://libuv.org/ class=external-link target=_blank rel=noopener>libuv</a>).
I figured that it would be a high performer and I wasn&rsquo;t wrong.</p><p>Waitress was the underdog for me but&mldr; Wow!
I was really blown away by its performance.
It even gives NGINX a run for its money by smashing through 67% of its requests per second.
Even more amazing is that Waitress is a pure-Python implementation with zero external dependencies.
Talk about good engineering.</p><p>I&rsquo;m definitely going to spend some time reading the source of Waitress to understand more about how they achieve such impressive performance.
Thankfully, they have a well-written <a href=https://docs.pylonsproject.org/projects/waitress/en/stable/design.html class=external-link target=_blank rel=noopener>design overview</a> which will be a good place to start.</p><h1 id=conclusion>Conclusion
<a class=heading-link href=#conclusion><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>On the surface, it looks like Waitress should be my web server of choice.
The performance is great (as we&rsquo;ve seen) and it supports a &ldquo;bring your own socket&rdquo; model of initialization.
However, another subtle requirement is that the server must support graceful restarts (wait for existing traffic to finish before exiting).
As far as I can tell, Waitress does not support this while Gunicorn does.</p><p>Plus, when deployed behind NGINX, all of these web servers designs and libraries perform the same: around 450 RPS.
This means that I can use a simple Gunicorn configuration and not even bother with gevent.
NGINX must be doing some sort of buffering magic to make this happen.</p><p>It really is something!</p></div><footer></footer></article></section></div><footer class=footer><section class=container>Â©
2024
Andrew Dailey
Â·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>